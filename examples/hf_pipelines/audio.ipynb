{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioToText Pipeline Demo\n",
    "\n",
    "## This notebook demonstrates how to use the TextToAudio pipeline to convert text into speech using a text-to-speech model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datasets import load_dataset\n",
    "from huggingface_pipelines.speech import AudioToTextHFPipeline, AudioPipelineConfig\n",
    "from huggingface_pipelines.dataset import DatasetConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.13k/8.13k [00:00<00:00, 37.8MB/s]\n",
      "Downloading readme: 100%|██████████| 14.4k/14.4k [00:00<00:00, 32.7MB/s]\n",
      "Downloading extra modules: 100%|██████████| 3.44k/3.44k [00:00<00:00, 16.1MB/s]\n",
      "Downloading extra modules: 100%|██████████| 60.9k/60.9k [00:00<00:00, 832kB/s]\n",
      "Downloading data: 100%|██████████| 12.2k/12.2k [00:00<00:00, 26.4MB/s]\n",
      "Downloading data:   0%|          | 0/24 [02:12<?, ?files/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "        dataset_name=\"mozilla-foundation/common_voice_11_0\",\n",
    "        dataset_split=\"test\",\n",
    "        trust_remote_code = True,\n",
    "        config = \"en\",\n",
    "    )\n",
    "\n",
    "dataset = dataset_config.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AudioPipelineConfig(\n",
    "    dataset_name=\"common_voice\",\n",
    "    columns=[\"audio\"]\n",
    ").with_overwrites({\n",
    "    \"encoder_model\": \"sonar_speech_encoder_eng\",\n",
    "    \"decoder_model\": \"text_sonar_basic_decoder\",\n",
    "    \"target_lang\": \"eng_Latn\",\n",
    "    \"batch_size\": 1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_file_name\": \"transcription_results\",\n",
    "    \"take\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AudioToTextHFPipeline(config)\n",
    "dataset = pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original, transcribed in zip(dataset['sentence'][:5], processed_dataset['audio_transcribed'][:5]):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Transcribed: {transcribed}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Play Audio Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_and_transcription(audio, transcription, sample_rate):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveshow(audio, sr=sample_rate)\n",
    "    plt.title(f'Audio Waveform\\nTranscription: {transcription}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "    ipd.display(ipd.Audio(audio, rate=sample_rate))\n",
    "\n",
    "sample_audio = dataset['audio'][0]['array']\n",
    "sample_rate = dataset['audio'][0]['sampling_rate']\n",
    "sample_transcription = processed_dataset['audio_transcribed'][0]\n",
    "\n",
    "plot_audio_and_transcription(sample_audio, sample_transcription, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Run MetricAnalyzer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_config = MetricPipelineConfig(\n",
    "    dataset_name=\"common_voice\",\n",
    "    dataset_split=\"test\",\n",
    "    columns=[\"audio_transcribed\"]\n",
    ").with_overwrites({\n",
    "    \"batch_size\": 32,\n",
    "    \"device\": \"cpu\",\n",
    "    \"metric_name\": \"wer\",  # Word Error Rate\n",
    "    \"low_score_threshold\": 0.5,  # Adjust as needed\n",
    "    \"output_file_name\": \"transcription_metrics\",\n",
    "    \"take\": 100\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_pipeline = MetricAnalyzerPipeline(metric_config)\n",
    "dataset = metric_pipeline(transcribed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(evaluated_dataset['wer'], bins=20)\n",
    "plt.title('Distribution of Word Error Rates')\n",
    "plt.xlabel('Word Error Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=metric_config.low_score_threshold, color='r', linestyle='--', label='Low Score Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average WER: {sum(evaluated_dataset['wer']) / len(evaluated_dataset['wer']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Low-Scoring Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_scoring = evaluated_dataset.filter(lambda x: x['wer'] > metric_config.low_score_threshold)\n",
    "print(f\"Number of low-scoring samples: {len(low_scoring)}\")\n",
    "\n",
    "for sample in low_scoring[:5]:\n",
    "    print(f\"Original: {sample['sentence']}\")\n",
    "    print(f\"Transcribed: {sample['audio_transcribed']}\")\n",
    "    print(f\"WER: {sample['wer']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
